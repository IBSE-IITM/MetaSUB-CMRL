{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "This notebook walks through the analysis of the section `Taxonomic classification and comparative analysis of distinct microbial signatures`  \n",
    "\n",
    "\n",
    "## Supplementary tables created\n",
    "- Table S1: Number of samples sequenced per metro station \n",
    "- Table S2: Total reads per sample after removing human and mouse reads \n",
    "- Table S3: Total detected species across different species categories \n",
    "- Table S4: Number of samples used per Cities from MetaSUB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "bounds = [0.97, 0.8] \n",
    "peri = 0.15\n",
    "MIN_SAMPLES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_list = [(0.1,\"1\"),(0.01,\"01\"),(0.001,\"001\"),(0.0001,\"0001\")]\n",
    "#prevalence_list = [(0.001,\"001\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.path.join(\n",
    "    \"..\",\"data\"\n",
    ")\n",
    "CHENNAI_DATASET = os.path.join(DATASET,\"Chennai_data\")\n",
    "CORE_TAXA = os.path.join(\n",
    "    DATASET,\n",
    "    \"MetaSUB_data\",\n",
    ")\n",
    "RESULTS = os.path.join(\n",
    "    \"..\",\"results\"\n",
    ")\n",
    "SUPPLEMENTARY = os.path.join(RESULTS, \"supplementary\")\n",
    "os.makedirs(SUPPLEMENTARY, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_extractor(df, bounds=[], col_not_to_include = []):\n",
    "    taxa_df = copy.copy(df)\n",
    "    taxa_list = [i for i in taxa_df.columns if i not in col_not_to_include]\n",
    "\n",
    "    for i in taxa_list:\n",
    "        taxa_df[i] = taxa_df[i] > 0\n",
    "        taxa_df[i] = taxa_df[i] * 1\n",
    "\n",
    "    v = taxa_df[taxa_list].sum() / taxa_df.shape[0]\n",
    "    v = v.to_frame()\n",
    "    v = v.reset_index()\n",
    "    v.rename(columns={\"index\": \"Species\", 0: \"prevalence\"}, inplace=True)\n",
    "    #v[\"Species\"] = v[\"Species\"].str.replace(\"prev_\", \"\")\n",
    "    v.sort_values('prevalence', ascending = False, inplace = True)\n",
    "    core_df = v.query(\"prevalence > @bounds[0]\")\n",
    "    sub_core_df = v.query(\"prevalence > @bounds[1] and prevalence < @bounds[0]\")\n",
    "    peripheral_df = v.query(\"prevalence < @peri and prevalence > 0.00001\")\n",
    "    v = v.rename(columns={'prevalence':'prevalence'})\n",
    "    return v, core_df, sub_core_df, peripheral_df\n",
    "\n",
    "def merger_function(taxa_df, metadata_df):\n",
    "    taxa_temp = copy.copy(taxa_df)\n",
    "    taxa_meta = copy.copy(metadata_df)\n",
    "    taxa_temp = taxa_temp.merge(\n",
    "        taxa_meta, how=\"inner\", left_on=\"Samples\", right_on=\"uuid\"\n",
    "    )\n",
    "    return taxa_temp\n",
    "\n",
    "def filter_all_zero(df1, col_not_to_include=[\"Samples\"]):\n",
    "    df = copy.copy(df1)\n",
    "    col_list = [i for i in df.columns if i not in col_not_to_include]\n",
    "    temp = df[col_list].sum().to_frame()#.reset_index()\n",
    "    temp.rename(columns={0: \"non_zeros\"}, inplace=True)\n",
    "    #print(\"before \", df.shape)\n",
    "    ## The name of the species which are not having any read mapped. These columns can be dropped\n",
    "    df = df.drop(\n",
    "        columns=list(temp.query(\"non_zeros<=0.000001\").index.values)\n",
    "    )  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplementary Table S1 : Number of samples sequenced per metro station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Samples</th>\n",
       "      <th>DNA/RNA</th>\n",
       "      <th>sample type(N/P/E)</th>\n",
       "      <th>location name</th>\n",
       "      <th>sublocation</th>\n",
       "      <th>select object/sampling place</th>\n",
       "      <th>select surface material</th>\n",
       "      <th>select ground level</th>\n",
       "      <th>Traffic at the sampling location</th>\n",
       "      <th>temperature</th>\n",
       "      <th>date and time</th>\n",
       "      <th>Sampling Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>is_kiosk</th>\n",
       "      <th>is_Rod</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>368257968</td>\n",
       "      <td>DNA</td>\n",
       "      <td>E</td>\n",
       "      <td>Washermenpet</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Bannister</td>\n",
       "      <td>Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12:37</td>\n",
       "      <td>8/12/2021</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2021</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368273379</td>\n",
       "      <td>DNA</td>\n",
       "      <td>E</td>\n",
       "      <td>Washermenpet</td>\n",
       "      <td>Train</td>\n",
       "      <td>Rod</td>\n",
       "      <td>Metal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12:44</td>\n",
       "      <td>8/12/2021</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2021</td>\n",
       "      <td>Other</td>\n",
       "      <td>Rod</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>368281532</td>\n",
       "      <td>DNA</td>\n",
       "      <td>E</td>\n",
       "      <td>Central</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Ticket Counter</td>\n",
       "      <td>Ceramic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12:57</td>\n",
       "      <td>8/12/2021</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2021</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>368281511</td>\n",
       "      <td>DNA</td>\n",
       "      <td>E</td>\n",
       "      <td>Central</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Kiosk</td>\n",
       "      <td>Glass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13:00</td>\n",
       "      <td>8/12/2021</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2021</td>\n",
       "      <td>Kiosk</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>368281318</td>\n",
       "      <td>DNA</td>\n",
       "      <td>E</td>\n",
       "      <td>Central</td>\n",
       "      <td>Metro</td>\n",
       "      <td>Ticket Counter</td>\n",
       "      <td>Ceramic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13:17</td>\n",
       "      <td>8/12/2021</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2021</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Samples DNA/RNA sample type(N/P/E) location name sublocation  \\\n",
       "0  368257968     DNA                  E  Washermenpet       Metro   \n",
       "1  368273379     DNA                  E  Washermenpet       Train   \n",
       "2  368281532     DNA                  E       Central       Metro   \n",
       "3  368281511     DNA                  E       Central       Metro   \n",
       "4  368281318     DNA                  E       Central       Metro   \n",
       "\n",
       "  select object/sampling place select surface material  select ground level  \\\n",
       "0                    Bannister                   Metal                  NaN   \n",
       "1                          Rod                   Metal                  NaN   \n",
       "2               Ticket Counter                 Ceramic                  NaN   \n",
       "3                        Kiosk                   Glass                  NaN   \n",
       "4               Ticket Counter                 Ceramic                  NaN   \n",
       "\n",
       "  Traffic at the sampling location  temperature date and time Sampling Date  \\\n",
       "0                              NaN          NaN         12:37     8/12/2021   \n",
       "1                              NaN          NaN         12:44     8/12/2021   \n",
       "2                              NaN          NaN         12:57     8/12/2021   \n",
       "3                              NaN          NaN         13:00     8/12/2021   \n",
       "4                              NaN          NaN         13:17     8/12/2021   \n",
       "\n",
       "  Month  Year is_kiosk is_Rod  2021  2022  \n",
       "0   Dec  2021    Other  Other     1     0  \n",
       "1   Dec  2021    Other    Rod     1     0  \n",
       "2   Dec  2021    Other  Other     1     0  \n",
       "3   Dec  2021    Kiosk  Other     1     0  \n",
       "4   Dec  2021    Other  Other     1     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chennai_metadata = pd.read_csv(os.path.join(CHENNAI_DATASET, 'metadata_processed.csv'))\n",
    "chennai_metadata[[\"2021\",\"2022\"]] = pd.get_dummies(chennai_metadata['Year'])*1\n",
    "chennai_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "supp_t_s1 = chennai_metadata[[\"location name\",\"2021\",\"2022\"]].groupby(['location name']).sum()\n",
    "total_row = supp_t_s1.sum().to_frame().T \n",
    "total_row['location name'] = 'Total'\n",
    "supp_t_s1 = supp_t_s1.reset_index() \n",
    "supp_t_s1 = pd.concat([supp_t_s1.reset_index(drop=True), total_row], ignore_index=True)\n",
    "supp_t_s1.to_csv(os.path.join(SUPPLEMENTARY, \"supplementary_table_s1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplementary Table S2 : Total reads per sample after removing human and mouse reads \n",
    "This is derived from the lof files of Kraken2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp_table_s2 = pd.read_csv(os.path.join(CHENNAI_DATASET, \"Chennai_kraken2_log.csv\"))\n",
    "supp_table_s2.to_csv(os.path.join(SUPPLEMENTARY, \"supplementary_table_s2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Chennai analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Threshold : 0.1\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "No of MetaSUB sampled (2515, 237)\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:16<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 0 Where 0 are unique to the city\n",
      "Working with Threshold : 0.01\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "No of MetaSUB sampled (2515, 1064)\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:57<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 0 Where 0 are unique to the city\n",
      "Working with Threshold : 0.001\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "No of MetaSUB sampled (2515, 3085)\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [02:44<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 0 Where 0 are unique to the city\n",
      "Working with Threshold : 0.0001\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "No of MetaSUB sampled (2515, 5824)\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [05:30<00:00, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 0 Where 0 are unique to the city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Load the Chennai bracken results\n",
    "chennai_species_df = pd.read_csv(os.path.join(CHENNAI_DATASET, 'bracken_species_non_human_processed.csv'))\n",
    "chennai_species_df.set_index('Samples', inplace= True)\n",
    "\n",
    "supplementary_s3 = {\"Species Category\":[\"Core\",\"Sub-core\",\"Peripheral\", \"Unique-core\"],}\n",
    "\n",
    "for threshold,folder in prevalence_list:\n",
    "    print(f\"Working with Threshold : {threshold}\")\n",
    "    ## Make the directories\n",
    "    RESULT_threshold = os.path.join(RESULTS, 'microbial_signatures', folder)\n",
    "    os.makedirs(RESULT_threshold, exist_ok = True)\n",
    "    ## Chennai \n",
    "    s_list = chennai_species_df.columns\n",
    "    chennai_species_df[s_list] = chennai_species_df[s_list].applymap(lambda x: 0 if x < threshold else x)\n",
    "    chennai_species_df = filter_all_zero(chennai_species_df)\n",
    "    full_prevalence_df, chennai_core_df, chennai_sub_core_df, chennai_peripheral_df = prevalence_extractor(chennai_species_df, bounds=bounds, col_not_to_include = ['Samples'])\n",
    "\n",
    "    core_species_dict = {'Chennai': { \n",
    "                                    'core':list(chennai_core_df[\"Species\"].values),\n",
    "                                    'sub_core':list(chennai_sub_core_df[\"Species\"].values),\n",
    "                                    'peripheral':list(chennai_peripheral_df[\"Species\"].values) \n",
    "                                  } \n",
    "                                  }\n",
    "    full_prevalence_df.to_csv(os.path.join(RESULT_threshold,'species_prevalence.csv'),index=False)\n",
    "    ## ROW\n",
    "    METADATA = os.path.join(CORE_TAXA,\"Filtered_complete_metadata.csv\",)\n",
    "    metadata_df = pd.read_csv(METADATA)\n",
    "    metasub_species = pd.read_csv(os.path.join(CORE_TAXA, 'Filtered_bracken_species_non_human_processed.csv'))\n",
    "\n",
    "    s_list = [i for i in metasub_species.columns if i != 'Samples']\n",
    "    print('Core apply threshold')\n",
    "    metasub_species[s_list] = metasub_species[s_list].applymap(lambda x: 0 if x < threshold else x)\n",
    "    #metasub_species = metasub_species.apply(abd_threshold, axis=1)\n",
    "    print('Core filter all zero')\n",
    "    metasub_species = filter_all_zero(metasub_species)\n",
    "    print('Core merge metadata')\n",
    "    metasub_species = merger_function(metasub_species, metadata_df)\n",
    "    \n",
    "    ## Filter the cities based on number of samples\n",
    "    vc = metasub_species['city'].value_counts()\n",
    "    filtered_vc = vc[vc >= MIN_SAMPLES]\n",
    "    \n",
    "    print(\"Iterate through each cities\")\n",
    "    for city in tqdm(filtered_vc.index):\n",
    "        city_species_df = metasub_species.query('city == @city')\n",
    "        col_not_to_include=[\"Samples\",\"uuid\",\n",
    "                            \"core_project\",\n",
    "                            \"project\",\n",
    "                            \"city\",\n",
    "                            \"surface_material\",\n",
    "                            \"continent\",\n",
    "                            \"surface_ontology_fine\",\n",
    "                            \"control_type\",]\n",
    "        _, _core_df, _sub_core_df, _peripheral_df = prevalence_extractor(city_species_df, bounds=bounds, col_not_to_include = col_not_to_include)\n",
    "        core_species_dict[city] = { \n",
    "                                        'core':list(_core_df[\"Species\"].values),\n",
    "                                        'sub_core':list(_sub_core_df[\"Species\"].values),\n",
    "                                        'peripheral':list(_peripheral_df[\"Species\"].values) \n",
    "                                      } \n",
    "                                      \n",
    "    \n",
    "    with open(os.path.join(RESULT_threshold,'all_city_core.json'), 'w') as fp:\n",
    "        json.dump(core_species_dict, fp)\n",
    "\n",
    "    ## Get the unique core for Chennai\n",
    "    other_city = [i for i in core_species_dict.keys() if i != 'Chennai']\n",
    "\n",
    "    unique_taxas_dict = {}\n",
    "    #for city_of_int in all_cities:\n",
    "    city_of_int = 'Chennai'\n",
    "    all_other_city_core_subcore_set = set()\n",
    "    for city in other_city:\n",
    "        #if city != city_of_int:\n",
    "        all_other_city_core_subcore_set = all_other_city_core_subcore_set.union(\n",
    "            core_species_dict[city][\"core\"]\n",
    "        )\n",
    "        all_other_city_core_subcore_set = all_other_city_core_subcore_set.union(\n",
    "            core_species_dict[city][\"sub_core\"]\n",
    "        )\n",
    "\n",
    "    c_difference = list(\n",
    "        set(core_species_dict[city_of_int][\"core\"]).difference(all_other_city_core_subcore_set)\n",
    "    )\n",
    "    unique_taxas_dict[city_of_int] = c_difference\n",
    "    print(\n",
    "        f\" Total {city_of_int.capitalize()} {'core'}\",\n",
    "        len(core_species_dict[city_of_int][\"core\"]),\n",
    "        \"Where\",\n",
    "        c_difference.__len__(),\n",
    "        \"are unique to the city\",\n",
    "    )\n",
    "    temp = pd.DataFrame.from_dict({\"Chennai_spl\": unique_taxas_dict[\"Chennai\"]})\n",
    "    temp.to_csv(os.path.join(RESULT_threshold, \"chennai_spl_core_considered_core_subcore.csv\"), index=False)\n",
    "\n",
    "    supplementary_s3[f\"Relative abundance > 0.{folder}\"] = [len(core_species_dict[\"Chennai\"][\"core\"]),\n",
    "                                                            len(core_species_dict[\"Chennai\"][\"sub_core\"]),\n",
    "                                                            len(core_species_dict[\"Chennai\"][\"peripheral\"]),\n",
    "                                                            len(unique_taxas_dict[\"Chennai\"]),]\n",
    "pd.DataFrame.from_dict(supplementary_s3).to_csv(os.path.join(SUPPLEMENTARY, \"supplementary_table_s3.csv\"), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supplementary s4 : Number of samples used per Cities from MetaSUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2515"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Actually 38, total sampled 2515\n",
    "metasub_species['city'].value_counts().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of MetaSUB sampled 2461 cities 31\n"
     ]
    }
   ],
   "source": [
    "filtered_vc.to_csv(os.path.join(SUPPLEMENTARY, \"supplementary_table_s4.csv\"))\n",
    "## sum to get the total number \n",
    "print(\"No of MetaSUB sampled\", filtered_vc.sum(), \"cities\", len(filtered_vc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chennai-MetaSUB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
