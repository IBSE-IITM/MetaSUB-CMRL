{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "bounds = [0.97, 0.8] \n",
    "peri = 0.15\n",
    "MIN_SAMPLES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_list = [(0.1,\"1\"),(0.01,\"01\"),(0.001,\"001\"),(0.0001,\"0001\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.path.join(\n",
    "    \"..\",\"..\",\"data\"\n",
    ")\n",
    "CHENNAI_DATASET = os.path.join(DATASET,\"Chennai_data\")\n",
    "CORE_TAXA = os.path.join(\n",
    "    DATASET,\n",
    "    \"MetaSUB_data\",\n",
    ")\n",
    "RESULTS = os.path.join(\n",
    "    \"..\",\"..\",\"results\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevalence_extractor(df, bounds=[], col_not_to_include = []):\n",
    "    taxa_df = copy.copy(df)\n",
    "    taxa_list = [i for i in taxa_df.columns if i not in col_not_to_include]\n",
    "\n",
    "    for i in taxa_list:\n",
    "        taxa_df[i] = taxa_df[i] > 0\n",
    "        taxa_df[i] = taxa_df[i] * 1\n",
    "\n",
    "    v = taxa_df[taxa_list].sum() / taxa_df.shape[0]\n",
    "    v = v.to_frame()\n",
    "    v = v.reset_index()\n",
    "    v.rename(columns={\"index\": \"Species\", 0: \"prevalence\"}, inplace=True)\n",
    "    #v[\"Species\"] = v[\"Species\"].str.replace(\"prev_\", \"\")\n",
    "    v.sort_values('prevalence', ascending = False, inplace = True)\n",
    "    core_df = v.query(\"prevalence > @bounds[0]\")\n",
    "    sub_core_df = v.query(\"prevalence > @bounds[1] and prevalence < @bounds[0]\")\n",
    "    peripheral_df = v.query(\"prevalence < @peri and prevalence > 0.00001\")\n",
    "    v = v.rename(columns={'prevalence':'prevalence'})\n",
    "    return v, core_df, sub_core_df, peripheral_df\n",
    "\n",
    "def merger_function(taxa_df, metadata_df):\n",
    "    taxa_temp = copy.copy(taxa_df)\n",
    "    taxa_meta = copy.copy(metadata_df)\n",
    "    taxa_temp = taxa_temp.merge(\n",
    "        taxa_meta, how=\"inner\", left_on=\"Samples\", right_on=\"uuid\"\n",
    "    )\n",
    "    return taxa_temp\n",
    "\n",
    "def filter_all_zero(df1, col_not_to_include=[\"Samples\"]):\n",
    "    df = copy.copy(df1)\n",
    "    col_list = [i for i in df.columns if i not in col_not_to_include]\n",
    "    temp = df[col_list].sum().to_frame()#.reset_index()\n",
    "    temp.rename(columns={0: \"non_zeros\"}, inplace=True)\n",
    "    #print(\"before \", df.shape)\n",
    "    ## The name of the species which are not having any read mapped. These columns can be dropped\n",
    "    df = df.drop(\n",
    "        columns=list(temp.query(\"non_zeros<=0.000001\").index.values)\n",
    "    )  \n",
    "\n",
    "    #print(\"after \", df.shape)\n",
    "    #print('Removed',list(temp.query(\"non_zeros<=0.000001\").index.values))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Chennai bracken results\n",
    "chennai_species_df = pd.read_csv(os.path.join(CHENNAI_DATASET, 'bracken_species_non_human_processed.csv'))\n",
    "chennai_species_df.set_index('Samples', inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Chennai analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Threshold : 0.0001\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [04:35<00:00,  8.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 464 Where 11 are unique to the city\n",
      "Working with Threshold : 0.001\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [02:12<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 27 Where 8 are unique to the city\n",
      "Working with Threshold : 0.01\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:45<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 2 Where 0 are unique to the city\n",
      "Working with Threshold : 0.1\n",
      "Core apply threshold\n",
      "Core filter all zero\n",
      "Core merge metadata\n",
      "Iterate through each cities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:11<00:00,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Chennai core 0 Where 0 are unique to the city\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "supplementary_s3 = {\"Species Category\":[\"Core\",\"Sub-core\",\"Peripheral\", \"Unique-core\"],}\n",
    "\n",
    "for threshold,folder in prevalence_list:\n",
    "    print(f\"Working with Threshold : {threshold}\")\n",
    "    ## Chennai \n",
    "    s_list = chennai_species_df.columns\n",
    "    chennai_species_df[s_list] = chennai_species_df[s_list].applymap(lambda x: 0 if x < threshold else x)\n",
    "    chennai_species_df = filter_all_zero(chennai_species_df)\n",
    "    full_prevalence_df, chennai_core_df, chennai_sub_core_df, chennai_peripheral_df = prevalence_extractor(chennai_species_df, bounds=bounds, col_not_to_include = ['Samples'])\n",
    "\n",
    "    core_species_dict = {'Chennai': { \n",
    "                                    'core':list(chennai_core_df[\"Species\"].values),\n",
    "                                    'sub_core':list(chennai_sub_core_df[\"Species\"].values),\n",
    "                                    'peripheral':list(chennai_peripheral_df[\"Species\"].values) \n",
    "                                  } \n",
    "                                  }\n",
    "    ## ROW\n",
    "    METADATA = os.path.join(CORE_TAXA,\"Filtered_complete_metadata.csv\",)\n",
    "    metadata_df = pd.read_csv(METADATA)\n",
    "    metasub_species = pd.read_csv(os.path.join(CORE_TAXA, 'Filtered_bracken_species_non_human_processed.csv'))\n",
    "\n",
    "    s_list = [i for i in metasub_species.columns if i != 'Samples']\n",
    "    print('Core apply threshold')\n",
    "    metasub_species[s_list] = metasub_species[s_list].applymap(lambda x: 0 if x < threshold else x)\n",
    "    #metasub_species = metasub_species.apply(abd_threshold, axis=1)\n",
    "    print('Core filter all zero')\n",
    "    metasub_species = filter_all_zero(metasub_species)\n",
    "    print('Core merge metadata')\n",
    "    metasub_species = merger_function(metasub_species, metadata_df)\n",
    "\n",
    "    ## Filter the cities based on number of samples\n",
    "    vc = metasub_species['city'].value_counts()\n",
    "    filtered_vc = vc[vc >= MIN_SAMPLES]\n",
    "\n",
    "    print(\"Iterate through each cities\")\n",
    "    for city in tqdm(filtered_vc.index):\n",
    "        city_species_df = metasub_species.query('city == @city')\n",
    "        col_not_to_include=[\"Samples\",\"uuid\",\n",
    "                            \"core_project\",\n",
    "                            \"project\",\n",
    "                            \"city\",\n",
    "                            \"surface_material\",\n",
    "                            \"continent\",\n",
    "                            \"surface_ontology_fine\",\n",
    "                            \"control_type\",]\n",
    "        _, _core_df, _sub_core_df, _peripheral_df = prevalence_extractor(city_species_df, bounds=bounds, col_not_to_include = col_not_to_include)\n",
    "        core_species_dict[city] = { \n",
    "                                        'core':list(_core_df[\"Species\"].values),\n",
    "                                        'sub_core':list(_sub_core_df[\"Species\"].values),\n",
    "                                        'peripheral':list(_peripheral_df[\"Species\"].values) \n",
    "                                      } \n",
    "                                      \n",
    "    ## Make the directories\n",
    "    RESULT_threshold = os.path.join(RESULTS, 'microbial_signatures', folder)\n",
    "    os.makedirs(RESULT_threshold, exist_ok = True)\n",
    "    with open(os.path.join(RESULT_threshold,'all_city_core.json'), 'w') as fp:\n",
    "        json.dump(core_species_dict, fp)\n",
    "\n",
    "    ## Get the unique core for Chennai\n",
    "    other_city = [i for i in core_species_dict.keys() if i != 'Chennai']\n",
    "\n",
    "    unique_taxas_dict = {}\n",
    "    #for city_of_int in all_cities:\n",
    "    city_of_int = 'Chennai'\n",
    "    all_other_city_core_subcore_set = set()\n",
    "    for city in other_city:\n",
    "        #if city != city_of_int:\n",
    "        all_other_city_core_subcore_set = all_other_city_core_subcore_set.union(\n",
    "            core_species_dict[city][\"core\"]\n",
    "        )\n",
    "        all_other_city_core_subcore_set = all_other_city_core_subcore_set.union(\n",
    "            core_species_dict[city][\"sub_core\"]\n",
    "        )\n",
    "\n",
    "    c_difference = list(\n",
    "        set(core_species_dict[city_of_int][\"core\"]).difference(all_other_city_core_subcore_set)\n",
    "    )\n",
    "    unique_taxas_dict[city_of_int] = c_difference\n",
    "    print(\n",
    "        f\" Total {city_of_int.capitalize()} {'core'}\",\n",
    "        len(core_species_dict[city_of_int][\"core\"]),\n",
    "        \"Where\",\n",
    "        c_difference.__len__(),\n",
    "        \"are unique to the city\",\n",
    "    )\n",
    "    temp = pd.DataFrame.from_dict({\"Chennai_spl\": unique_taxas_dict[\"Chennai\"]})\n",
    "    temp.to_csv(os.path.join(RESULT_threshold, \"chennai_spl_core_considered_core_subcore.csv\"), index=False)\n",
    "\n",
    "    supplementary_s3[f\"Relative abundance > 0.{folder}\"] = [len(core_species_dict[\"Chennai\"][\"core\"]),\n",
    "                                                            len(core_species_dict[\"Chennai\"][\"sub_core\"]),\n",
    "                                                            len(core_species_dict[\"Chennai\"][\"peripheral\"]),\n",
    "                                                            len(unique_taxas_dict[\"Chennai\"]),]\n",
    "pd.DataFrame.from_dict(supplementary_s3).to_csv(os.path.join(RESULTS, 'microbial_signatures', \"supplementary_s3.csv\"), index=False)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chennai-MetaSUB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
